{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import findspark\n",
    "findspark.init() #jupyter notebook에서 -> 사용하면 pyspark를 일반 library처럼 사용 가능.\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()     #기존에 열려있는 session가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from make_table.ipynb\n",
      "table oil done\n",
      "table PyeongtaekWeather done\n",
      "table AnseongWeather done\n",
      "table ChuncheonWeather done\n",
      "table JeongeupWeather done\n",
      "table HampyeongWeather done\n",
      "table SangjuWeather done\n",
      "table GimhaeWeather done\n",
      "table HamyangWeather done\n",
      "table priceTable done\n",
      "=============================Create Table Everyday DONE=============================\n",
      "table priceIndex done\n",
      "=============================Create Table Month DONE=============================\n",
      "table riceProduction done\n",
      "table onionProduction done\n",
      "table cucumberProduction done\n",
      "table greenOnionProduction done\n",
      "table pumpkinProduction done\n",
      "=============================Create Table Year DONE=============================\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from make_table import CreateEveryday, CreateMonth, CreateYear\n",
    "\n",
    "CreateEveryday()    #### 하루에 한번씩\n",
    "CreateMonth()     #### 달에 한번씩\n",
    "CreateYear()     #### 년에 한번씩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "\n",
    "\n",
    "crops_dict = {'오이':[['취청(50개)','가시계통(1kg)','다다기계통(100개)'],['중품','상품']],\n",
    "              '양파':[['햇양파(1kg)','양파(1kg)','수입(1kg)'],['중품','상품']],\n",
    "              '파':[['대파(1kg)','쪽파(1kg)'],['중품','상품']],\n",
    "              '호박':[['애호박(20개)','쥬키니(1kg)'],['중품','상품']],\n",
    "              '쌀':[['일반계(1kg)','햇일반계(1kg)'],['중품','상품']]}\n",
    "\n",
    "area_dict = {'오이':'PyeongtaekWeather', '양파':'HampyeongWeather', '파':'AnseongWeather', '호박':'ChuncheonWeather', '쌀':'JeongeupWeather'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "element_for_predict = {'오이':{'취청(50개)':{'중품':{},'상품':{}}, '가시계통(1kg)':{'중품':{},'상품':{}}, '다다기계통(100개)':{'중품':{},'상품':{}}},\n",
    "                           '양파':{'햇양파(1kg)':{'중품':{},'상품':{}},'양파(1kg)':{'중품':{},'상품':{}},'수입(1kg)':{'중품':{},'상품':{}}},\n",
    "                           '파':{'대파(1kg)':{'중품':{},'상품':{}}, '쪽파(1kg)':{'중품':{},'상품':{}}},\n",
    "                           '호박':{'애호박(20개)':{'중품':{},'상품':{}}, '쥬키니(1kg)':{'중품':{},'상품':{}}},\n",
    "                           '쌀': {'일반계(1kg)':{'중품':{},'상품':{}}, '햇일반계(1kg)':{'중품':{},'상품':{}}}}\n",
    "\n",
    "\n",
    "for item in crops_dict.keys():\n",
    "    for kind in crops_dict[item][0]:\n",
    "        for rank in crops_dict[item][1]:\n",
    "            if item=='오이':\n",
    "                element_for_predict[item][kind][rank]={'priceIndex':['채소및과실','기타운송장비','농업및건설용기계','식료품'],'weather':['dayAvgTa','dayAvgRhm']}\n",
    "            elif item=='양파':\n",
    "                element_for_predict[item][kind][rank]={'priceIndex':['채소및과실','전력가스및증기','식료품','비료및농약'],'weather':['dayAvgRhm','dayAvgTa','dayAvgWs']}\n",
    "            elif item=='파':\n",
    "                element_for_predict[item][kind][rank]={'priceIndex':['채소및과실','기타운송장비','음식점및숙박서비스','농업및건설용기계'],'weather':['dayAvgRhm']}\n",
    "            elif item=='호박' and kind=='애호박(20개)':\n",
    "                element_for_predict[item][kind][rank]={'priceIndex':['채소및과실','기타운송장비','음료품','식료품'],'weather':['dayAvgTa']}\n",
    "            elif item=='호박' and kind=='쥬키니(1kg)':\n",
    "                element_for_predict[item][kind][rank]={'priceIndex':['채소및과실','기타운송장비','음료품','식료품'],'weather':[]}\n",
    "            elif item=='쌀':\n",
    "                element_for_predict[item][kind][rank]={'priceIndex':['곡물및식량작물','수도폐기물처리및재활용서비스','음식점및숙박서비스','사업지원서비스','장비용품및지식재산권임대'],'weather':[]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/오이_취청(50개)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/오이_취청(50개)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/오이_가시계통(1kg)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/오이_가시계통(1kg)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/오이_다다기계통(100개)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/오이_다다기계통(100개)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/양파_양파(1kg)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/양파_양파(1kg)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양파_수입(1kg)_중품_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/파_대파(1kg)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/파_대파(1kg)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/파_쪽파(1kg)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/파_쪽파(1kg)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/호박_애호박(20개)_중품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================regression_model/호박_애호박(20개)_상품_model DONE======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호박_쥬키니(1kg)_중품_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호박_쥬키니(1kg)_상품_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쌀_일반계(1kg)_중품_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쌀_일반계(1kg)_상품_model\n"
     ]
    }
   ],
   "source": [
    "for item in crops_dict.keys():\n",
    "    for kind in crops_dict[item][0]:\n",
    "        for rank in crops_dict[item][1]:\n",
    "            try:\n",
    "                if element_for_predict[item][kind][rank]['priceIndex'] != []:\n",
    "                    priceIndex_select_str = []\n",
    "                    for i in range(0, len(element_for_predict[item][kind][rank]['priceIndex'])):\n",
    "                        if i==len(element_for_predict[item][kind][rank]['priceIndex'])-1:\n",
    "                            priceIndex_select_str.append(\"priceIndex.`{}`\".format(element_for_predict[item][kind][rank]['priceIndex'][i]))\n",
    "                        else:\n",
    "                            priceIndex_select_str.append(\"priceIndex.`{}`,\".format(element_for_predict[item][kind][rank]['priceIndex'][i]))\n",
    "                    priceIndex_select_str = ''.join(priceIndex_select_str)\n",
    "\n",
    "\n",
    "                    query = \"SELECT {3},priceTable.price,priceTable.timestamp\\\n",
    "                                FROM priceIndex RIGHT JOIN priceTable\\\n",
    "                                ON priceIndex.`날짜`==priceTable.timestamp\\\n",
    "                                WHERE priceTable.item_name='{0}' AND priceTable.rank='{2}' AND priceTable.kind_name='{1}' AND priceTable.timestamp NOT IN ('-') \\\n",
    "                                ORDER BY priceTable.timestamp ASC\"\n",
    "\n",
    "                    priceIndex_df = spark.sql(query.format(item,kind,rank,priceIndex_select_str))\n",
    "\n",
    "                    for i in element_for_predict[item][kind][rank]['priceIndex']:\n",
    "                        priceIndex_df = priceIndex_df.withColumn(i, func.last(i, True).over(Window.partitionBy(func.month('timestamp')).orderBy('timestamp').rowsBetween(-sys.maxsize, 0))).orderBy(priceIndex_df.timestamp.asc())\n",
    "\n",
    "\n",
    "                if element_for_predict[item][kind][rank]['weather'] != []:\n",
    "                    weather_select_str = []\n",
    "                    for i in range(0, len(element_for_predict[item][kind][rank]['weather'])):\n",
    "                        if i==len(element_for_predict[item][kind][rank]['weather'])-1:\n",
    "                            weather_select_str.append(\"{0}.`{1}`\".format(area_dict[item],element_for_predict[item][kind][rank]['weather'][i]))\n",
    "                        else:\n",
    "                            weather_select_str.append(\"{0}.`{1}`,\".format(area_dict[item],element_for_predict[item][kind][rank]['weather'][i]))\n",
    "                    weather_select_str = ''.join(weather_select_str)    \n",
    "\n",
    "                    query = \"SELECT {0}, {1}.date AS `timestamp` \\\n",
    "                    FROM {1} WHERE wrnCount=0 \\\n",
    "                    ORDER BY `timestamp`\"\n",
    "\n",
    "                    weather_df = spark.sql(query.format(weather_select_str, area_dict[item]))\n",
    "\n",
    "                if element_for_predict[item][kind][rank]['priceIndex']!=[] and element_for_predict[item][kind][rank]['weather']!=[]:\n",
    "                    regression_df = priceIndex_df.join(weather_df, \"timestamp\", \"inner\").orderBy(\"timestamp\")\n",
    "\n",
    "                elif element_for_predict[item][kind][rank]['weather']==[]:\n",
    "                    regression_df = priceIndex_df.orderBy(\"timestamp\")\n",
    "                elif element_for_predict[item][kind][rank]['priceIndex']==[]:\n",
    "                    regression_df = weather_df.orderBy(\"timestamp\")     \n",
    "\n",
    "                spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "                regression_pdf = regression_df.select(\"*\").toPandas()\n",
    "                regression_pdf.dropna(subset = [\"price\"], inplace=True)\n",
    "\n",
    "                x_element = []\n",
    "                x_element = list(element_for_predict[item][kind][rank]['priceIndex'])\n",
    "                x_element.extend(element_for_predict[item][kind][rank]['weather'])\n",
    "\n",
    "                x = regression_pdf[x_element]\n",
    "                y = regression_pdf[['price']]\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)\n",
    "\n",
    "                #     predict_dict[item][kind][rank] = True\n",
    "                model = LinearRegression()\n",
    "                model.fit(x_train, y_train)\n",
    "\n",
    "                filename = 'regression_model/{0}_{1}_{2}_model'.format(item,kind,rank)\n",
    "                pickle.dump(model, open(filename, 'wb'))\n",
    "                print(\"======================{} DONE======================\".format(filename))\n",
    "            \n",
    "            except:\n",
    "                print('error: {0}_{1}_{2}'.format(item,kind,rank))\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
